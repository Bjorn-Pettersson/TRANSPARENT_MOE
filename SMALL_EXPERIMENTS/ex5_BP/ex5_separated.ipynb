{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe960413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_moe_allpretrain_experiment_topk_seq.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "CONFIG = {\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_experts\": 4,\n",
    "    \"expert_layers\": 1,\n",
    "    \"pretrain_epochs\": 2,        # light pretraining\n",
    "    \"pretrain_n_per_domain\": 300,\n",
    "    \"moe_train_epochs\": 6,\n",
    "    \"moe_n_per_domain\": 600,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"seed_list\": [42],\n",
    "    \"use_pretrained_embeddings\": False,\n",
    "    \"freeze_pretrained_for\": 0,\n",
    "    \"entropy_reg\": 0.0,\n",
    "    \"top_k\": 2,                  # top-k for sequence-level routing\n",
    "    \"save_dir\": \"moe_allpretrain_results_topk_seq\"\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"save_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------- Dataset (math, law, biokem, stroy) --------------------\n",
    "class SpecializedTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=128, dataset_type='mixed', n_per_domain=500):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        n = n_per_domain\n",
    "\n",
    "        domains = {\n",
    "            'math': self._generate_math_samples(n),\n",
    "            'law': self._generate_law_samples(n),\n",
    "            'biokem': self._generate_biokem_samples(n),\n",
    "            'stroy': self._generate_stroy_samples(n)\n",
    "        }\n",
    "\n",
    "        if dataset_type in domains:\n",
    "            self.data = domains[dataset_type]\n",
    "            self.labels = [dataset_type] * len(self.data)\n",
    "        elif dataset_type == 'mixed':\n",
    "            for domain, samples in domains.items():\n",
    "                self.data.extend(samples)\n",
    "                self.labels.extend([domain] * len(samples))\n",
    "        else:\n",
    "            raise ValueError(\"dataset_type must be one of the domains or 'mixed'\")\n",
    "\n",
    "        combined = list(zip(self.data, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        if combined:\n",
    "            self.data, self.labels = zip(*combined)\n",
    "        else:\n",
    "            self.data, self.labels = [], []\n",
    "\n",
    "    # --- generators ---\n",
    "    def _generate_math_samples(self, n):\n",
    "        templates = [\n",
    "            \"Consider the function f(x) = {poly}. Compute f'({x0}).\",\n",
    "            \"Prove that the sequence a_n = {recurrence} converges and find its limit.\",\n",
    "            \"Let A be a matrix with eigenvalues {eig1} and {eig2}; determine if A is diagonalizable.\",\n",
    "            \"Evaluate the integral âˆ« {expr} dx between {a} and {b}.\",\n",
    "            \"Solve the differential equation y' + {p} y = {q} with initial condition y({x0}) = {y0}.\"\n",
    "        ]\n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            s = random.choice(templates).format(\n",
    "                poly=f\"{random.randint(1,5)}x^{random.randint(1,4)} + {random.randint(0,5)}x + {random.randint(0,3)}\",\n",
    "                x0=random.randint(0,5),\n",
    "                recurrence=f\"a_n = a_{{n-1}}/{random.randint(2,5)} + {random.randint(0,3)}\",\n",
    "                eig1=random.randint(-3,3),\n",
    "                eig2=random.randint(-3,3),\n",
    "                expr=f\"{random.randint(1,5)}*x^{random.randint(0,3)}\",\n",
    "                a=random.randint(-2,0),\n",
    "                b=random.randint(1,4),\n",
    "                p=random.randint(0,5),\n",
    "                q=random.randint(0,5),\n",
    "                y0=random.randint(0,3)\n",
    "            )\n",
    "            samples.append(s)\n",
    "        return samples\n",
    "\n",
    "    def _generate_law_samples(self, n):\n",
    "        templates = [\n",
    "            \"Under the {act}, courts have held that {principle} applies when {condition}.\",\n",
    "            \"The contract was voidable due to {defect}; relevant precedent includes {case}.\",\n",
    "            \"A plaintiff must establish {element1}, {element2}, and {element3} to succeed in a negligence claim.\",\n",
    "            \"Statutory interpretation favored the narrow reading because the legislative history showed {reason}.\"\n",
    "        ]\n",
    "        acts = [\"Contracts Act\", \"Tort Law Reform\", \"Evidence Code\"]\n",
    "        principles = [\"strict liability\", \"reasonable person standard\", \"proportionality\"]\n",
    "        cases = [\"Case A v. B\", \"R v. Smith\", \"Brown v. Board\"]\n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            s = random.choice(templates).format(\n",
    "                act=random.choice(acts),\n",
    "                principle=random.choice(principles),\n",
    "                condition=random.choice([\"harm was foreseeable\", \"parties had unequal bargaining power\"]),\n",
    "                defect=random.choice([\"misrepresentation\", \"undue influence\", \"illegality\"]),\n",
    "                case=random.choice(cases),\n",
    "                element1=\"duty\",\n",
    "                element2=\"breach\",\n",
    "                element3=\"causation\",\n",
    "                reason=random.choice([\"clear legislative intent\", \"policy concerns\"])\n",
    "            )\n",
    "            samples.append(s)\n",
    "        return samples\n",
    "\n",
    "    def _generate_biokem_samples(self, n):\n",
    "        # 'biokem' = biochemical / biochemical kinetics style text\n",
    "        templates = [\n",
    "            \"The enzyme follows Michaelis-Menten kinetics with Vmax = {vmax} and Km = {km}.\",\n",
    "            \"Spectroscopy revealed absorbance peaks at {nm} nm indicating the presence of {compound}.\",\n",
    "            \"The reaction rate increased with substrate concentration until it reached Vmax, suggesting {interpretation}.\",\n",
    "            \"Cell cultures treated with {agent} showed upregulated expression of {gene}.\"\n",
    "        ]\n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            s = random.choice(templates).format(\n",
    "                vmax=round(random.uniform(0.5, 10.0), 2),\n",
    "                km=round(random.uniform(0.01, 5.0), 3),\n",
    "                nm=random.randint(200, 700),\n",
    "                compound=random.choice([\"heme\", \"chlorophyll\", \"aromatic amino acid\"]),\n",
    "                interpretation=random.choice([\"enzyme saturation\", \"allosteric regulation\"]),\n",
    "                agent=random.choice([\"DrugX\", \"inhibitorY\"]),\n",
    "                gene=random.choice([\"GeneA\", \"ProteinB\"])\n",
    "            )\n",
    "            samples.append(s)\n",
    "        return samples\n",
    "\n",
    "    def _generate_stroy_samples(self, n):\n",
    "        # 'stroy' is treated as short creative story-like lines\n",
    "        templates = [\n",
    "            \"He found the letter on the kitchen table and the edges were {adj}.\",\n",
    "            \"Rain came down in {pattern} as she walked toward the {place}.\",\n",
    "            \"The city at night smelled of {smell} and old concrete, and the neon was {color}.\",\n",
    "            \"She remembered her childhood by the sea: the {object}, the {sound}, and the blue horizon.\"\n",
    "        ]\n",
    "        adjectives = [\"torn\", \"crisp\", \"watermarked\"]\n",
    "        patterns = [\"sheets\", \"a steady drizzle\", \"a sudden downpour\"]\n",
    "        places = [\"station\", \"apartment\", \"pier\"]\n",
    "        smells = [\"salt\", \"fried bread\", \"wet asphalt\"]\n",
    "        colors = [\"flickering\", \"blinding\", \"muted\"]\n",
    "        objects = [\"guitar\", \"toy boat\", \"old tin\"]\n",
    "        sounds = [\"distant laughter\", \"a train's whistle\", \"seagulls\"]\n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            s = random.choice(templates).format(\n",
    "                adj=random.choice(adjectives),\n",
    "                pattern=random.choice(patterns),\n",
    "                place=random.choice(places),\n",
    "                smell=random.choice(smells),\n",
    "                color=random.choice(colors),\n",
    "                object=random.choice(objects),\n",
    "                sound=random.choice(sounds)\n",
    "            )\n",
    "            samples.append(s)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'domain': label\n",
    "        }\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "class TransformerExpert(nn.Module):\n",
    "    def __init__(self, hidden_size=CONFIG[\"hidden_size\"], num_layers=CONFIG[\"expert_layers\"]):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=4,\n",
    "            dim_feedforward=hidden_size*2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        out = self.transformer(hidden_states, src_key_padding_mask=src_key_padding_mask)\n",
    "        return out\n",
    "\n",
    "class RouterSequenceTopK(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-level router that computes gating per sequence (per example),\n",
    "    selects top-k experts per sequence and returns sparse normalized weights.\n",
    "    Outputs:\n",
    "      - seq_weights: [B, E] (sparse; zeros for non-topk)\n",
    "      - expanded_weights: [B, L, E] (same weights expanded across sequence length)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=CONFIG[\"hidden_size\"], num_experts=CONFIG[\"num_experts\"], top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size//2, num_experts)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # hidden_states: [B, L, H]\n",
    "        # pool to sequence vector (mean pooling over valid tokens)\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1, keepdim=True)  # [B,1]\n",
    "            summed = (hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1)  # [B,H]\n",
    "            pooled = summed / (lengths.clamp(min=1).to(hidden_states.dtype))\n",
    "        else:\n",
    "            pooled = hidden_states.mean(dim=1)  # [B,H]\n",
    "\n",
    "        logits = self.gate(pooled)  # [B, E]\n",
    "\n",
    "        # select top-k indices per sequence\n",
    "        topk_vals, topk_idx = torch.topk(logits, k=min(self.top_k, self.num_experts), dim=-1)  # [B, k]\n",
    "        # Build sparse mask and compute softmax over just the topk logits\n",
    "        B = logits.size(0)\n",
    "        device = logits.device\n",
    "        sparse_logits = torch.full_like(logits, float('-inf'))  # [B, E]\n",
    "        arange = torch.arange(B, device=device).unsqueeze(-1)\n",
    "        sparse_logits[arange, topk_idx] = topk_vals\n",
    "        # now safe softmax (will be zero where -inf)\n",
    "        seq_weights = F.softmax(sparse_logits, dim=-1)  # [B, E], zeros outside topk\n",
    "        # expand across sequence length\n",
    "        L = hidden_states.size(1)\n",
    "        expanded = seq_weights.unsqueeze(1).expand(-1, L, -1)  # [B, L, E]\n",
    "        return seq_weights, expanded  # sequence weights and expanded token-level weights\n",
    "\n",
    "class TransformerMoE(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=CONFIG[\"hidden_size\"], num_experts=CONFIG[\"num_experts\"], num_layers=CONFIG[\"expert_layers\"], top_k=2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.router = RouterSequenceTopK(hidden_size, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([TransformerExpert(hidden_size, num_layers) for _ in range(num_experts)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, return_routing=False):\n",
    "        # input_ids: [B, L]\n",
    "        hidden = self.embeddings(input_ids)  # [B, L, H]\n",
    "        seq_weights, routing_expanded = self.router(hidden, attention_mask)  # [B, E], [B, L, E]\n",
    "\n",
    "        # run each expert on the same hidden inputs\n",
    "        expert_outputs = []\n",
    "        for e in range(self.num_experts):\n",
    "            out = self.experts[e](hidden, attention_mask)  # [B, L, H]\n",
    "            expert_outputs.append(out)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=0)  # [E, B, L, H]\n",
    "\n",
    "        # prepare routing for combination: want [E, B, L, 1]\n",
    "        # routing_expanded: [B, L, E] -> permute to [E, B, L]\n",
    "        r = routing_expanded.permute(2, 0, 1).unsqueeze(-1)  # [E, B, L, 1]\n",
    "        mixed = (expert_outputs * r).sum(dim=0)  # [B, L, H]\n",
    "\n",
    "        logits = self.lm_head(mixed)  # [B, L, V]\n",
    "        if return_routing:\n",
    "            # return both sequence-level sparse weights and expanded token-level weights\n",
    "            return logits, seq_weights, routing_expanded\n",
    "        return logits\n",
    "\n",
    "    def load_pretrained_expert(self, expert_idx, state_dict):\n",
    "        self.experts[expert_idx].load_state_dict(state_dict)\n",
    "        print(f\"Loaded pretrained expert into slot {expert_idx}\")\n",
    "\n",
    "# -------------------- Pretrain each expert lightly --------------------\n",
    "def pretrain_all_experts_light(tokenizer, vocab_size, device, seed=42):\n",
    "    \"\"\"\n",
    "    Pretrain each expert lightly on its own domain and return lists of expert state_dicts and embedding state_dicts.\n",
    "    \"\"\"\n",
    "    print(\"\\n== PHASE 1: Lightly pretrain ALL experts (per-domain) ==\")\n",
    "    domain_order = ['math', 'law', 'biokem', 'stroy']\n",
    "    pre_states = []\n",
    "    pre_embeddings = []\n",
    "\n",
    "    for i, domain in enumerate(domain_order):\n",
    "        print(f\"\\nPretraining expert {i} on domain '{domain}' (light)...\")\n",
    "        # generate domain-specific samples\n",
    "        tmp = SpecializedTextDataset(tokenizer, dataset_type='mixed', n_per_domain=0)\n",
    "        if domain == 'math':\n",
    "            samples = tmp._generate_math_samples(CONFIG[\"pretrain_n_per_domain\"])\n",
    "        elif domain == 'law':\n",
    "            samples = tmp._generate_law_samples(CONFIG[\"pretrain_n_per_domain\"])\n",
    "        elif domain == 'biokem':\n",
    "            samples = tmp._generate_biokem_samples(CONFIG[\"pretrain_n_per_domain\"])\n",
    "        elif domain == 'stroy':\n",
    "            samples = tmp._generate_stroy_samples(CONFIG[\"pretrain_n_per_domain\"])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown domain\")\n",
    "\n",
    "        labels = [domain] * len(samples)\n",
    "\n",
    "        class SmallTextDS(Dataset):\n",
    "            def __init__(self, samples, labels, tokenizer):\n",
    "                self.samples = samples\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "            def __len__(self): return len(self.samples)\n",
    "            def __getitem__(self, idx):\n",
    "                encoded = self.tokenizer(self.samples[idx], max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
    "                return {'input_ids': encoded['input_ids'].squeeze(0), 'attention_mask': encoded['attention_mask'].squeeze(0), 'domain': self.labels[idx]}\n",
    "\n",
    "        dataset = SmallTextDS(samples, labels, tokenizer)\n",
    "        loader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        expert = TransformerExpert(CONFIG[\"hidden_size\"], CONFIG[\"expert_layers\"]).to(device)\n",
    "        emb = nn.Embedding(vocab_size, CONFIG[\"hidden_size\"]).to(device)\n",
    "        lm_head = nn.Linear(CONFIG[\"hidden_size\"], vocab_size).to(device)\n",
    "\n",
    "        opt = optim.Adam(list(expert.parameters()) + list(emb.parameters()) + list(lm_head.parameters()), lr=CONFIG[\"lr\"])\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        for epoch in range(CONFIG[\"pretrain_epochs\"]):\n",
    "            expert.train()\n",
    "            total = 0.0\n",
    "            for batch in loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                hidden = emb(input_ids)\n",
    "                out = expert(hidden, attention_mask)\n",
    "                logits = lm_head(out)\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = input_ids[..., 1:].contiguous()\n",
    "                loss = criterion(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total += loss.item()\n",
    "            print(f\"  Epoch {epoch+1}/{CONFIG['pretrain_epochs']} avg loss: {total/len(loader):.4f}\")\n",
    "\n",
    "        pre_states.append(expert.state_dict())\n",
    "        pre_embeddings.append(emb.state_dict())\n",
    "\n",
    "    print(\"\\nAll experts lightly pretrained.\")\n",
    "    return pre_states, pre_embeddings\n",
    "\n",
    "# -------------------- Train MoE --------------------\n",
    "def train_moe_allpretrained(pre_states, pre_embeddings, tokenizer, vocab_size, device, seed=42):\n",
    "    print(\"\\n== PHASE 2: Initialize MoE with pretrained experts and train on mixed data (top-k seq routing) ==\")\n",
    "    mixed = SpecializedTextDataset(tokenizer, dataset_type='mixed', n_per_domain=CONFIG[\"moe_n_per_domain\"])\n",
    "    loader = DataLoader(mixed, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    moe = TransformerMoE(vocab_size, CONFIG[\"hidden_size\"], CONFIG[\"num_experts\"], CONFIG[\"expert_layers\"], top_k=CONFIG[\"top_k\"]).to(device)\n",
    "\n",
    "    # Load expert states into slots\n",
    "    for i, st in enumerate(pre_states):\n",
    "        moe.load_pretrained_expert(i, st)\n",
    "\n",
    "    if CONFIG[\"use_pretrained_embeddings\"]:\n",
    "        avg_weight = None\n",
    "        for d in pre_embeddings:\n",
    "            w = d['weight']\n",
    "            if avg_weight is None:\n",
    "                avg_weight = w.clone()\n",
    "            else:\n",
    "                avg_weight += w\n",
    "        avg_weight /= len(pre_embeddings)\n",
    "        moe.embeddings.weight.data.copy_(avg_weight)\n",
    "        print(\"Loaded averaged pretrained embeddings into MoE embeddings.\")\n",
    "    else:\n",
    "        print(\"MoE embeddings left random (use_pretrained_embeddings=False).\")\n",
    "\n",
    "    opt = optim.Adam(moe.parameters(), lr=CONFIG[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    freeze_k = CONFIG[\"freeze_pretrained_for\"]\n",
    "\n",
    "    for epoch in range(CONFIG[\"moe_train_epochs\"]):\n",
    "        if epoch < freeze_k:\n",
    "            for p in moe.experts:\n",
    "                for param in p.parameters():\n",
    "                    param.requires_grad = False\n",
    "            print(f\"Epoch {epoch+1}: pretrained experts frozen.\")\n",
    "        elif epoch == freeze_k:\n",
    "            for p in moe.experts:\n",
    "                for param in p.parameters():\n",
    "                    param.requires_grad = True\n",
    "            print(f\"Epoch {epoch+1}: unfroze pretrained experts; full fine-tuning now.\")\n",
    "\n",
    "        moe.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            logits, seq_routing, expanded_routing = moe(input_ids, attention_mask, return_routing=True)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "\n",
    "            # entropy regularizer on the sequence-level routing distribution (encourage spread)\n",
    "            if CONFIG[\"entropy_reg\"] > 0.0:\n",
    "                # seq_routing: [B, E]\n",
    "                entropy = - (seq_routing * (seq_routing + 1e-12).log()).sum(dim=-1).mean()\n",
    "                loss = loss + CONFIG[\"entropy_reg\"] * (-entropy)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['moe_train_epochs']} avg loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "    print(\"MoE training finished.\")\n",
    "    return moe\n",
    "\n",
    "# -------------------- Analysis --------------------\n",
    "def analyze_routing(moe, tokenizer, device):\n",
    "    print(\"\\n== PHASE 3: Analyze sequence-level top-k routing (domain <-> expert alignment) ==\")\n",
    "    test_samples = {\n",
    "        'math': [\n",
    "            \"Compute the derivative of f(x) = 3x^3 + 2x - 5 at x = 2.\",\n",
    "            \"Find the limit of the sequence defined by a_n = a_{n-1}/2 + 1 with a_0 = 1.\",\n",
    "            \"Evaluate the integral of x^2 from 0 to 3.\",\n",
    "            \"Is the matrix with eigenvalues 1 and 2 diagonalizable?\",\n",
    "            \"Solve y' + 2y = 3 with initial condition y(0)=0.\"\n",
    "        ],\n",
    "        'law': [\n",
    "            \"Under the Contracts Act, misrepresentation can make a contract voidable.\",\n",
    "            \"A negligence claim requires duty, breach, and causation to be established.\",\n",
    "            \"The court relied on precedent in R v. Smith to interpret the statute.\",\n",
    "            \"Public policy concerns influenced the judgment on enforceability.\",\n",
    "            \"The defendant argued undue influence rendered the agreement void.\"\n",
    "        ],\n",
    "        'biokem': [\n",
    "            \"The enzyme kinetics gave Vmax = 5.2 and Km = 0.15 consistent with saturation.\",\n",
    "            \"Absorbance peaked at 280 nm suggesting aromatic residues present.\",\n",
    "            \"Substrate concentration increase led to rate saturation at high levels.\",\n",
    "            \"Cells treated with inhibitorY showed decreased expression of ProteinB.\",\n",
    "            \"Michaelis-Menten parameters were estimated from Lineweaver-Burk plots.\"\n",
    "        ],\n",
    "        'stroy': [\n",
    "            \"She opened the letter and the edges were crisp like old paper.\",\n",
    "            \"Rain fell in sheets as he ran toward the pier.\",\n",
    "            \"The city smelled of salt and neon flickered above the wet streets.\",\n",
    "            \"He remembered a toy boat and the sound of seagulls from childhood.\",\n",
    "            \"The kitchen table held a cup and a folded map with faded ink.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    domain_order = ['math', 'law', 'biokem', 'stroy']\n",
    "    routing_stats = defaultdict(list)\n",
    "    seq_topk_counts = defaultdict(Counter)  # count which experts are in top-k for each domain\n",
    "\n",
    "    moe.eval()\n",
    "    with torch.no_grad():\n",
    "        for domain, samples in test_samples.items():\n",
    "            for text in samples:\n",
    "                enc = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
    "                input_ids = enc['input_ids'].to(device)\n",
    "                attention_mask = enc['attention_mask'].to(device)\n",
    "                logits, seq_routing, expanded_routing = moe(input_ids, attention_mask, return_routing=True)\n",
    "                # seq_routing: [B, E], batch size 1\n",
    "                seq_r = seq_routing[0].cpu().numpy()\n",
    "                routing_stats[domain].append(seq_r)\n",
    "                # which are top-k (non-zero in seq_r typically)\n",
    "                topk_indices = np.flatnonzero(seq_r > 0)\n",
    "                for idx in topk_indices:\n",
    "                    seq_topk_counts[domain][int(idx)] += 1\n",
    "\n",
    "    # Build usage matrix (average % weight per expert across sequences)\n",
    "    usage = np.zeros((len(domain_order), moe.num_experts))\n",
    "    for i, d in enumerate(domain_order):\n",
    "        arr = np.array(routing_stats[d]) if routing_stats[d] else np.zeros((1, moe.num_experts))\n",
    "        usage[i] = arr.mean(axis=0) * 100\n",
    "\n",
    "    # Print per-domain expert breakdown\n",
    "    print(\"\\nAverage expert usage (%) by domain (sequence-level):\")\n",
    "    for i, d in enumerate(domain_order):\n",
    "        print(f\"\\n{d.upper()}:\")\n",
    "        for e in range(moe.num_experts):\n",
    "            print(f\"  Expert {e}: {usage[i, e]:.2f}%\")\n",
    "\n",
    "    # Compute \"matching expert\" statistics:\n",
    "    matches = []\n",
    "    for i, d in enumerate(domain_order):\n",
    "        counts = seq_topk_counts[d]\n",
    "        total = sum(counts.values()) if counts else 0\n",
    "        # count how often canonical expert i appears in top-k for domain d\n",
    "        match_count = counts.get(i, 0)\n",
    "        frac = match_count / total if total > 0 else 0.0\n",
    "        matches.append((d, i, match_count, total, frac))\n",
    "        print(f\"\\nDomain '{d}': expert {i} present in top-k for {match_count}/{total} sequences = {frac*100:.1f}%\")\n",
    "\n",
    "    # Save CSV summary\n",
    "    csv_path = os.path.join(CONFIG[\"save_dir\"], f\"routing_summary_seed_seqtopk.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"domain\", \"matching_expert\", \"match_count\", \"total_sequences\", \"fraction\"])\n",
    "        for d, idx, mcount, tot, frac in matches:\n",
    "            writer.writerow([d, idx, mcount, tot, frac])\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(usage, annot=True, fmt='.1f', cmap='YlGnBu',\n",
    "                xticklabels=[f'E{e}' for e in range(moe.num_experts)],\n",
    "                yticklabels=[d.capitalize() for d in domain_order])\n",
    "    plt.title('Average Expert Usage (%) by Domain (sequence-level)')\n",
    "    plt.xlabel('Expert')\n",
    "    plt.ylabel('Domain')\n",
    "    plt.tight_layout()\n",
    "    imgpath = os.path.join(CONFIG[\"save_dir\"], \"usage_heatmap_seqtopk.png\")\n",
    "    plt.savefig(imgpath, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return usage, seq_topk_counts, matches\n",
    "\n",
    "# -------------------- Multi-seed runner --------------------\n",
    "def run_single_seed(seed):\n",
    "    print(f\"\\n\\n=== Running seed {seed} ===\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    pre_states, pre_embeds = pretrain_all_experts_light(tokenizer, vocab_size, device, seed)\n",
    "    moe = train_moe_allpretrained(pre_states, pre_embeds, tokenizer, vocab_size, device, seed)\n",
    "    usage, seq_topk_counts, matches = analyze_routing(moe, tokenizer, device)\n",
    "\n",
    "    fractions = {m[0]: m[4] for m in matches}\n",
    "    return fractions, usage\n",
    "\n",
    "def run_multi_seed():\n",
    "    all_results = []\n",
    "    usages = []\n",
    "    for seed in CONFIG[\"seed_list\"]:\n",
    "        fractions, usage = run_single_seed(seed)\n",
    "        all_results.append(fractions)\n",
    "        usages.append(usage)\n",
    "    # Aggregate\n",
    "    domains = ['math', 'law', 'biokem', 'stroy']\n",
    "    summary = {}\n",
    "    for d in domains:\n",
    "        vals = [res[d] for res in all_results]\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals, ddof=0)\n",
    "        summary[d] = {\"mean\": mean, \"std\": std, \"values\": vals}\n",
    "        print(f\"\\nDomain {d}: matched-expert fraction mean={mean*100:.1f}%, std={std*100:.1f}% over {len(vals)} seeds\")\n",
    "\n",
    "    # Save CSV\n",
    "    csv_out = os.path.join(CONFIG[\"save_dir\"], \"multi_seed_summary_seqtopk.csv\")\n",
    "    with open(csv_out, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"domain\", \"mean_frac\", \"std_frac\"] + [f\"seed_{s}\" for s in CONFIG[\"seed_list\"]])\n",
    "        for d in domains:\n",
    "            row = [d, summary[d][\"mean\"], summary[d][\"std\"]] + summary[d][\"values\"]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Save aggregated usage heatmap (mean across seeds)\n",
    "    mean_usage = np.mean(np.stack(usages, axis=0), axis=0)  # shape: seeds x domains x experts -> mean across seeds\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(mean_usage, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "                xticklabels=[f'E{e}' for e in range(CONFIG[\"num_experts\"])],\n",
    "                yticklabels=[d.capitalize() for d in domains])\n",
    "    plt.title('Mean Expert Usage (%) by Domain (across seeds) - seq top-k')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"save_dir\"], \"mean_usage_heatmap_seqtopk.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return summary\n",
    "\n",
    "# -------------------- Entry --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    summary = run_multi_seed()\n",
    "    print(\"\\nAll experiments done. Results saved to:\", CONFIG[\"save_dir\"])\n",
    "    print(\"Key outputs: CSV 'multi_seed_summary_seqtopk.csv' and heatmap images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3733f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
