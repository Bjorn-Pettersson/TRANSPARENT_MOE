{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6c3f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed 42 ===\n",
      "Pretrain math: epoch 1/2 avg loss 10.4968\n",
      "Pretrain math: epoch 2/2 avg loss 9.4091\n",
      "Pretrain legal: epoch 1/2 avg loss 10.4417\n",
      "Pretrain legal: epoch 2/2 avg loss 9.1212\n",
      "Pretrain biokem: epoch 1/2 avg loss 10.4311\n",
      "Pretrain biokem: epoch 2/2 avg loss 9.3107\n",
      "Pretrain storytelling: epoch 1/2 avg loss 10.4534\n",
      "Pretrain storytelling: epoch 2/2 avg loss 9.2630\n",
      "Loaded expert into slot 0\n",
      "Loaded expert into slot 1\n",
      "Loaded expert into slot 2\n",
      "Loaded expert into slot 3\n",
      "Epoch 1: experts frozen\n",
      "Epoch 1/4 avg loss 11.0638 avg_seq_ent 1.2067 avg_mean_gate_ent 1.2172\n",
      "Epoch 2: experts frozen\n",
      "Epoch 2/4 avg loss 6.9722 avg_seq_ent 0.2914 avg_mean_gate_ent 0.3070\n",
      "Epoch 3: experts unfrozen\n",
      "Epoch 3/4 avg loss 3.1954 avg_seq_ent 0.0516 avg_mean_gate_ent 0.0546\n",
      "Epoch 4/4 avg loss 1.6950 avg_seq_ent 0.0236 avg_mean_gate_ent 0.0250\n",
      "\n",
      "Analyzing routing...\n",
      "DDI: 0.9925249481201172\n",
      "Mean entropy per domain: {'math': 0.03245699033141136, 'legal': 0.08825654909014702, 'biokem': 0.054280150681734085, 'storytelling': 0.006147414290656646}\n",
      "\n",
      "Multi-seed DDI mean: 0.9925249481201172 std: 0.0\n",
      "\n",
      "Finished. Results saved to ex8_1_results\n",
      "Ex8_1 run summary (topics: math, legal, biokem, storytelling):\n",
      "Config: {'hidden_size': 128, 'num_experts': 4, 'expert_layers': 1, 'pretrain_epochs': 2, 'pretrain_n_per_domain': 300, 'moe_train_epochs': 4, 'moe_n_per_domain': 600, 'batch_size': 8, 'lr': 0.0001, 'seed_list': [42], 'use_pretrained_embeddings': False, 'freeze_pretrained_for': 2, 'entropy_reg_seq': 0.5, 'entropy_reg_mean': 0.5, 'temperature': 0.7, 'save_dir': 'ex8_1_results'}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Ex8_1 (modified): Sequence-Level Softmax Routing with Frozen Experts and Load-Balancing Loss\n",
    "CPU-friendly PyTorch notebook\n",
    "\n",
    "Changes:\n",
    " - Domains/topics replaced with: math, legal, biokem, storytelling\n",
    " - Synthetic generators and test samples updated accordingly\n",
    " - Rest of the pipeline kept intact\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Imports & Config\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "import seaborn as sns\n",
    "\n",
    "# Config (tweak for CPU speed)\n",
    "CONFIG = {\n",
    "    'hidden_size': 128,\n",
    "    'num_experts': 4,\n",
    "    'expert_layers': 1,\n",
    "    'pretrain_epochs': 2,\n",
    "    'pretrain_n_per_domain': 300,\n",
    "    'moe_train_epochs': 4,\n",
    "    'moe_n_per_domain': 600,\n",
    "    'batch_size': 8,\n",
    "    'lr': 1e-4,\n",
    "    'seed_list': [42],\n",
    "    'use_pretrained_embeddings': False,\n",
    "    'freeze_pretrained_for': 2,   # freeze expert weights for first k MOE epochs\n",
    "    'entropy_reg_seq': 0.5,      # weight to penalize per-sequence entropy (encourage low entropy)\n",
    "    'entropy_reg_mean': 0.5,     # weight to penalize mean-gate entropy across batch (encourage non-uniform mean)\n",
    "    'temperature': 0.7,\n",
    "    'save_dir': 'ex8_1_results'\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "# Reproducibility helper\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "# %%\n",
    "# Synthetic dataset (topics replaced: math, legal, biokem, storytelling)\n",
    "class SpecializedTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, domain=None, n_per_domain=300, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        def gen_math(n):\n",
    "            templates = [\n",
    "                \"We prove that {statement} by induction on {index}.\",\n",
    "                \"The {object} has eigenvalues given by {expr}, which implies {consequence}.\",\n",
    "                \"A simple algorithm for {problem} runs in O({complexity}) time and uses {space} space.\"\n",
    "            ]\n",
    "            statements = [\"the sequence converges\", \"the function is continuous\", \"the series is divergent\"]\n",
    "            objects = [\"matrix A\", \"operator T\", \"linear map\"]\n",
    "            exprs = [\"λ_i = i^2\", \"roots r_1 and r_2\", \"sinusoids at harmonics\"]\n",
    "            problems = [\"sorting\", \"graph traversal\", \"prime testing\"]\n",
    "            complexities = [\"n log n\", \"n^2\", \"poly(n)\"]\n",
    "            spaces = [\"O(1)\", \"O(n)\"]\n",
    "            out = []\n",
    "            for _ in range(n):\n",
    "                out.append(random.choice(templates).format(\n",
    "                    statement=random.choice(statements),\n",
    "                    index=random.choice([\"n\",\"k\"]),\n",
    "                    object=random.choice(objects),\n",
    "                    expr=random.choice(exprs),\n",
    "                    consequence=random.choice([\"stability\",\"boundedness\",\"uniqueness\"]),\n",
    "                    problem=random.choice(problems),\n",
    "                    complexity=random.choice(complexities),\n",
    "                    space=random.choice(spaces)\n",
    "                ))\n",
    "            return out\n",
    "\n",
    "        def gen_legal(n):\n",
    "            templates = [\n",
    "                \"The court found that the {contract_term} breached the statute under {law}.\",\n",
    "                \"Under {jurisdiction} law, the party may seek {remedy} for {violation}.\",\n",
    "                \"The precedent in {case} clarifies the interpretation of {doctrine}.\"\n",
    "            ]\n",
    "            contract_terms = [\"non-compete clause\", \"warranty provision\", \"confidentiality clause\"]\n",
    "            laws = [\"Section 12\", \"tort law\", \"consumer protection statutes\"]\n",
    "            jurisdictions = [\"federal\", \"state\", \"EU\"]\n",
    "            remedies = [\"injunctive relief\", \"damages\", \"restitution\"]\n",
    "            violations = [\"fraud\", \"negligence\", \"breach of contract\"]\n",
    "            cases = [\"Smith v. Jones\", \"R v. Corporation\"]\n",
    "            doctrines = [\"reasonable expectation\", \"strict liability\", \"duty of care\"]\n",
    "            out = []\n",
    "            for _ in range(n):\n",
    "                out.append(random.choice(templates).format(\n",
    "                    contract_term=random.choice(contract_terms),\n",
    "                    law=random.choice(laws),\n",
    "                    jurisdiction=random.choice(jurisdictions),\n",
    "                    remedy=random.choice(remedies),\n",
    "                    violation=random.choice(violations),\n",
    "                    case=random.choice(cases),\n",
    "                    doctrine=random.choice(doctrines)\n",
    "                ))\n",
    "            return out\n",
    "\n",
    "        def gen_biokem(n):\n",
    "            # \"biokem\" used as shorthand for biochemical/biokemistry style text\n",
    "            templates = [\n",
    "                \"The assay measured {molecule} concentration after {treatment}.\",\n",
    "                \"{enzyme} catalyzes the conversion of {substrate} to {product} with Km={km} and Vmax={vmax}.\",\n",
    "                \"Mass spectrometry revealed a peak corresponding to {compound} consistent with {interpretation}.\"\n",
    "            ]\n",
    "            molecules = [\"ATP\", \"glucose\", \"lactate\"]\n",
    "            treatments = [\"incubation\", \"heat shock\", \"drug exposure\"]\n",
    "            enzymes = [\"hexokinase\", \"polymerase\", \"lipase\"]\n",
    "            substrates = [\"glucose\", \"DNA\", \"lipid\"]\n",
    "            products = [\"G6P\", \"cDNA\", \"fatty acids\"]\n",
    "            kms = [\"0.1 mM\", \"5 µM\", \"50 µM\"]\n",
    "            vmaxs = [\"100 nmol/min\", \"2 µmol/min\", \"0.5 µmol/min\"]\n",
    "            compounds = [\"peptide A\", \"metabolite X\"]\n",
    "            interpretations = [\"metabolic activation\", \"post-translational modification\"]\n",
    "            out = []\n",
    "            for _ in range(n):\n",
    "                out.append(random.choice(templates).format(\n",
    "                    molecule=random.choice(molecules),\n",
    "                    treatment=random.choice(treatments),\n",
    "                    enzyme=random.choice(enzymes),\n",
    "                    substrate=random.choice(substrates),\n",
    "                    product=random.choice(products),\n",
    "                    km=random.choice(kms),\n",
    "                    vmax=random.choice(vmaxs),\n",
    "                    compound=random.choice(compounds),\n",
    "                    interpretation=random.choice(interpretations)\n",
    "                ))\n",
    "            return out\n",
    "\n",
    "        def gen_storytelling(n):\n",
    "            templates = [\n",
    "                \"When {character} entered the {place}, they found a {object} that changed everything.\",\n",
    "                \"The narrative follows {protagonist} as they face {conflict} and discover {reveal}.\",\n",
    "                \"A quiet scene at {setting} escalates into a confrontation about {theme}.\"\n",
    "            ]\n",
    "            characters = [\"an old sailor\", \"a young coder\", \"a grieving parent\"]\n",
    "            places = [\"abandoned pier\", \"neon-lit café\", \"derelict apartment\"]\n",
    "            objects = [\"tattered map\", \"burned letter\", \"silver locket\"]\n",
    "            protagonists = [\"Mara\", \"Jon\", \"the narrator\"]\n",
    "            conflicts = [\"a moral dilemma\", \"a long-buried secret\", \"a financial crisis\"]\n",
    "            reveals = [\"their true origin\", \"a hidden motive\", \"an unexpected ally\"]\n",
    "            settings = [\"dawn\", \"midnight\", \"a crowded market\"]\n",
    "            themes = [\"loss\", \"ambition\", \"betrayal\"]\n",
    "            out = []\n",
    "            for _ in range(n):\n",
    "                out.append(random.choice(templates).format(\n",
    "                    character=random.choice(characters),\n",
    "                    place=random.choice(places),\n",
    "                    object=random.choice(objects),\n",
    "                    protagonist=random.choice(protagonists),\n",
    "                    conflict=random.choice(conflicts),\n",
    "                    reveal=random.choice(reveals),\n",
    "                    setting=random.choice(settings),\n",
    "                    theme=random.choice(themes)\n",
    "                ))\n",
    "            return out\n",
    "\n",
    "        pools = {\n",
    "            'math': gen_math(n_per_domain),\n",
    "            'legal': gen_legal(n_per_domain),\n",
    "            'biokem': gen_biokem(n_per_domain),\n",
    "            'storytelling': gen_storytelling(n_per_domain)\n",
    "        }\n",
    "\n",
    "        if domain is None:\n",
    "            for d, samples in pools.items():\n",
    "                for s in samples:\n",
    "                    self.data.append(s)\n",
    "                    self.labels.append(d)\n",
    "        else:\n",
    "            for s in pools[domain]:\n",
    "                self.data.append(s)\n",
    "                self.labels.append(domain)\n",
    "\n",
    "        combined = list(zip(self.data, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.data, self.labels = zip(*combined)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc = tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {'input_ids': enc['input_ids'].squeeze(0), 'attention_mask': enc['attention_mask'].squeeze(0), 'domain': label}\n",
    "\n",
    "# %%\n",
    "# Model: experts + sequence-level softmax router + dense mixing\n",
    "class TransformerExpert(nn.Module):\n",
    "    def __init__(self, hidden_size=CONFIG['hidden_size'], num_layers=CONFIG['expert_layers']):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4, dim_feedforward=hidden_size*2, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, hidden, src_key_padding_mask=None):\n",
    "        return self.transformer(hidden, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "class SeqRouterSoftmax(nn.Module):\n",
    "    def __init__(self, hidden_size=CONFIG['hidden_size'], num_experts=CONFIG['num_experts'], hidden_mid=None):\n",
    "        super().__init__()\n",
    "        hidden_mid = hidden_mid or max(16, hidden_size//4)\n",
    "        self.net = nn.Sequential(nn.Linear(hidden_size, hidden_mid), nn.ReLU(), nn.Linear(hidden_mid, num_experts))\n",
    "\n",
    "    def forward(self, pooled, temperature=1.0):\n",
    "        logits = self.net(pooled) / (temperature + 1e-12)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return logits, probs\n",
    "\n",
    "class TransformerMoESeqSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=CONFIG['hidden_size'], num_experts=CONFIG['num_experts'], num_layers=CONFIG['expert_layers'], temperature=CONFIG['temperature']):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.router = SeqRouterSoftmax(hidden_size, num_experts)\n",
    "        self.experts = nn.ModuleList([TransformerExpert(hidden_size, num_layers) for _ in range(num_experts)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "        self.num_experts = num_experts\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, return_routing=False):\n",
    "        B, L = input_ids.shape\n",
    "        hidden = self.emb(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            pooled = (hidden * attention_mask.unsqueeze(-1)).sum(dim=1) / lengths\n",
    "        else:\n",
    "            pooled = hidden.mean(dim=1)\n",
    "\n",
    "        logits_seq, probs_seq = self.router(pooled, temperature=self.temperature)\n",
    "        # compute outputs for each expert\n",
    "        expert_outputs = []\n",
    "        for e in range(self.num_experts):\n",
    "            out = self.experts[e](hidden)\n",
    "            expert_outputs.append(out)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # [B, E, L, H]\n",
    "\n",
    "        # mix experts densely using probs_seq weights\n",
    "        probs_seq_exp = probs_seq.unsqueeze(-1).unsqueeze(-1)  # [B, E, 1, 1]\n",
    "        mixed = (expert_outputs * probs_seq_exp).sum(dim=1)  # [B, L, H]\n",
    "\n",
    "        logits = self.lm_head(mixed)\n",
    "        if return_routing:\n",
    "            return logits, probs_seq\n",
    "        return logits\n",
    "\n",
    "    def load_pretrained_expert(self, idx, state_dict):\n",
    "        self.experts[idx].load_state_dict(state_dict)\n",
    "        print(f\"Loaded expert into slot {idx}\")\n",
    "\n",
    "# %%\n",
    "# Pretrain experts lightly (Phase 1)\n",
    "def pretrain_expert_light(domain, tokenizer, vocab_size, device, n_samples=CONFIG['pretrain_n_per_domain'], epochs=CONFIG['pretrain_epochs']):\n",
    "    ds = SpecializedTextDataset(tokenizer, domain=domain, n_per_domain=n_samples)\n",
    "    loader = DataLoader(ds, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "    expert = TransformerExpert(CONFIG['hidden_size'], CONFIG['expert_layers']).to(device)\n",
    "    emb = nn.Embedding(vocab_size, CONFIG['hidden_size']).to(device)\n",
    "    lm_head = nn.Linear(CONFIG['hidden_size'], vocab_size).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(list(expert.parameters()) + list(emb.parameters()) + list(lm_head.parameters()), lr=CONFIG['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        expert.train()\n",
    "        total, count = 0.0, 0\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            hidden = emb(input_ids)\n",
    "            out = expert(hidden)\n",
    "            logits = lm_head(out)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item(); count += 1\n",
    "        print(f\"Pretrain {domain}: epoch {epoch+1}/{epochs} avg loss {total/count:.4f}\")\n",
    "\n",
    "    return expert.state_dict(), emb.state_dict()\n",
    "\n",
    "# %%\n",
    "# Transfer + train MoE with frozen experts and entropy penalties (Phase 2)\n",
    "def transfer_and_train_moe_softmax(pre_states, pre_embeds, tokenizer, vocab_size, device):\n",
    "    moe = TransformerMoESeqSoftmax(vocab_size, CONFIG['hidden_size'], CONFIG['num_experts'], CONFIG['expert_layers'], temperature=CONFIG['temperature']).to(device)\n",
    "\n",
    "    # load pretrained experts\n",
    "    for i, st in enumerate(pre_states):\n",
    "        moe.load_pretrained_expert(i, st)\n",
    "\n",
    "    # optionally average embeddings\n",
    "    if CONFIG['use_pretrained_embeddings']:\n",
    "        avg = None\n",
    "        for e in pre_embeds:\n",
    "            w = torch.tensor(e['weight'])\n",
    "            if avg is None: avg = w.clone()\n",
    "            else: avg += w\n",
    "        avg = (avg / len(pre_embeds)).to(moe.emb.weight.device)\n",
    "        moe.emb.weight.data.copy_(avg)\n",
    "\n",
    "    # dataset\n",
    "    mixed = SpecializedTextDataset(tokenizer, domain=None, n_per_domain=CONFIG['moe_n_per_domain'])\n",
    "    loader = DataLoader(mixed, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "    opt = torch.optim.Adam(moe.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    # freeze experts for initial epochs\n",
    "    freeze_k = CONFIG['freeze_pretrained_for']\n",
    "\n",
    "    for epoch in range(CONFIG['moe_train_epochs']):\n",
    "        if epoch < freeze_k:\n",
    "            for p in moe.experts.parameters(): p.requires_grad = False\n",
    "            print(f\"Epoch {epoch+1}: experts frozen\")\n",
    "        elif epoch == freeze_k:\n",
    "            for p in moe.experts.parameters(): p.requires_grad = True\n",
    "            print(f\"Epoch {epoch+1}: experts unfrozen\")\n",
    "\n",
    "        moe.train()\n",
    "        total, count = 0.0, 0\n",
    "        seq_entropies = []\n",
    "        mean_gate_entropies = []\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            logits, probs = moe(input_ids, attention_mask, return_routing=True)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "\n",
    "            # per-sequence entropy (encourage low entropy -> confident gating)\n",
    "            seq_entropy = - (probs * (probs + 1e-12).log()).sum(dim=-1).mean()\n",
    "            mean_gates = probs.mean(dim=0)\n",
    "            mean_gate_entropy = - (mean_gates * (mean_gates + 1e-12).log()).sum()\n",
    "\n",
    "            # add penalties (positive weights penalize high entropy)\n",
    "            loss = loss + CONFIG['entropy_reg_seq'] * seq_entropy + CONFIG['entropy_reg_mean'] * mean_gate_entropy\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item(); count += 1\n",
    "            seq_entropies.append(seq_entropy.item()); mean_gate_entropies.append(mean_gate_entropy.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['moe_train_epochs']} avg loss {total/count:.4f} avg_seq_ent {np.mean(seq_entropies):.4f} avg_mean_gate_ent {np.mean(mean_gate_entropies):.4f}\")\n",
    "\n",
    "    return moe\n",
    "\n",
    "# %%\n",
    "# Analysis: compute usage heatmap, DDI, entropy per domain\n",
    "\n",
    "def analyze_moe(moe, tokenizer, device):\n",
    "    print('\\nAnalyzing routing...')\n",
    "    test_samples = {\n",
    "        'math': [\n",
    "            \"We show the sequence is Cauchy and hence convergent in the given metric.\",\n",
    "            \"An algorithm for prime sieving runs in n log log n time in practice.\",\n",
    "            \"The eigenvalues of the symmetric matrix are all real and bounded.\"\n",
    "        ],\n",
    "        'legal': [\n",
    "            \"The contract's non-compete clause was struck down under state precedent.\",\n",
    "            \"Under EU law, the plaintiff may claim damages for the breach of statutory duty.\",\n",
    "            \"The appellate decision clarified the doctrine of reasonable expectation.\"\n",
    "        ],\n",
    "        'biokem': [\n",
    "            \"The enzyme kinetics showed a Michaelis-Menten curve with Km around 5 µM.\",\n",
    "            \"After drug exposure the assay detected increased levels of lactate.\",\n",
    "            \"Mass spectrometry confirmed the presence of peptide A consistent with activation.\"\n",
    "        ],\n",
    "        'storytelling': [\n",
    "            \"When Mara opened the letter she discovered a map leading to the abandoned pier.\",\n",
    "            \"A quiet morning escalated into a confrontation about betrayal at the café.\",\n",
    "            \"The protagonist faces a moral dilemma that forces a difficult choice.\"\n",
    "        ],\n",
    "    }\n",
    "    domains = ['math', 'legal', 'biokem', 'storytelling']\n",
    "\n",
    "    moe.eval()\n",
    "    routing_avgs = {d: [] for d in domains}\n",
    "    seq_entropies = {d: [] for d in domains}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d, samples in test_samples.items():\n",
    "            for s in samples:\n",
    "                enc = tokenizer(s, max_length=64, padding='max_length', truncation=True, return_tensors='pt')\n",
    "                input_ids = enc['input_ids'].to(device)\n",
    "                attention_mask = enc['attention_mask'].to(device)\n",
    "                logits, probs = moe(input_ids, attention_mask, return_routing=True)\n",
    "                probs_np = probs[0].cpu().numpy()\n",
    "                routing_avgs[d].append(probs_np)\n",
    "                ent = - (probs * (probs + 1e-12).log()).sum(dim=-1).item()\n",
    "                seq_entropies[d].append(ent)\n",
    "\n",
    "    usage = np.zeros((len(domains), moe.num_experts))\n",
    "    for i, d in enumerate(domains):\n",
    "        arr = np.array(routing_avgs[d]) if routing_avgs[d] else np.zeros((1, moe.num_experts))\n",
    "        usage[i] = arr.mean(axis=0) * 100.0\n",
    "\n",
    "    # DDI: mean of max per-row fraction (normalized 0..1)\n",
    "    row_max = usage.max(axis=1)\n",
    "    ddi = float(row_max.mean() / 100.0)\n",
    "\n",
    "    mean_ent = {d: float(np.mean(seq_entropies[d])) for d in domains}\n",
    "\n",
    "    # save CSV and heatmap\n",
    "    csv_path = os.path.join(CONFIG['save_dir'], 'ex8_1_usage.csv')\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['domain'] + [f'E{e}' for e in range(moe.num_experts)])\n",
    "        for i, d in enumerate(domains):\n",
    "            writer.writerow([d] + usage[i].tolist())\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(usage, annot=True, fmt='.1f', cmap='YlOrRd', xticklabels=[f'E{e}' for e in range(moe.num_experts)], yticklabels=[d.capitalize() for d in domains])\n",
    "    plt.title('Sequence-level Average Expert Usage (%) by Domain')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['save_dir'], 'ex8_1_usage_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return usage, ddi, mean_ent\n",
    "\n",
    "# %%\n",
    "# Multi-seed runner\n",
    "\n",
    "def run_single_seed(seed):\n",
    "    print(f\"\\n=== Seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "    device = torch.device('cpu')\n",
    "    tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tok.pad_token = tok.eos_token\n",
    "    global tokenizer\n",
    "    tokenizer = tok\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Phase 1: pretrain experts\n",
    "    domains = ['math', 'legal', 'biokem', 'storytelling']\n",
    "    pre_states, pre_embeds = [], []\n",
    "    for d in domains:\n",
    "        st, eb = pretrain_expert_light(d, tokenizer, vocab_size, device)\n",
    "        pre_states.append(st); pre_embeds.append(eb)\n",
    "\n",
    "    # Phase 2: transfer & train\n",
    "    moe = transfer_and_train_moe_softmax(pre_states, pre_embeds, tokenizer, vocab_size, device)\n",
    "\n",
    "    # Analyze\n",
    "    usage, ddi, mean_ent = analyze_moe(moe, tokenizer, device)\n",
    "    print('DDI:', ddi)\n",
    "    print('Mean entropy per domain:', mean_ent)\n",
    "\n",
    "    return {'usage': usage, 'ddi': ddi, 'entropy': mean_ent}\n",
    "\n",
    "\n",
    "def run_multi_seed():\n",
    "    results = []\n",
    "    for s in CONFIG['seed_list']:\n",
    "        res = run_single_seed(s)\n",
    "        results.append(res)\n",
    "    # aggregate DDI\n",
    "    ddis = [r['ddi'] for r in results]\n",
    "    print('\\nMulti-seed DDI mean:', np.mean(ddis), 'std:', np.std(ddis))\n",
    "    return results\n",
    "\n",
    "# %%\n",
    "# Entry\n",
    "if __name__ == '__main__':\n",
    "    res = run_multi_seed()\n",
    "    print('\\nFinished. Results saved to', CONFIG['save_dir'])\n",
    "\n",
    "# %%\n",
    "# Small textual summary for LLM consumption\n",
    "summary = []\n",
    "summary.append('Ex8_1 run summary (topics: math, legal, biokem, storytelling):')\n",
    "summary.append(f\"Config: {CONFIG}\")\n",
    "print('\\n'.join(summary))\n",
    "\n",
    "# End of notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
